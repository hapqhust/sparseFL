{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def iid_divide(l, g):\n",
    "    \"\"\"\n",
    "    https://github.com/TalwalkarLab/leaf/blob/master/data/utils/sample.py\n",
    "    divide list `l` among `g` groups\n",
    "    each group has either `int(len(l)/g)` or `int(len(l)/g)+1` elements\n",
    "    returns a list of groups\n",
    "    \"\"\"\n",
    "    num_elems = len(l)\n",
    "    group_size = int(len(l) / g)\n",
    "    num_big_groups = num_elems - g * group_size\n",
    "    num_small_groups = g - num_big_groups\n",
    "    glist = []\n",
    "    for i in range(num_small_groups):\n",
    "        glist.append(l[group_size * i: group_size * (i + 1)])\n",
    "    bi = group_size * num_small_groups\n",
    "    group_size += 1\n",
    "    for i in range(num_big_groups):\n",
    "        glist.append(l[bi + group_size * i:bi + group_size * (i + 1)])\n",
    "    return glist\n",
    "\n",
    "\n",
    "def split_list_by_indices(l, indices):\n",
    "    \"\"\"\n",
    "    divide list `l` given indices into `len(indices)` sub-lists\n",
    "    sub-list `i` starts from `indices[i]` and stops at `indices[i+1]`\n",
    "    returns a list of sub-lists\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    current_index = 0\n",
    "    for index in indices:\n",
    "        res.append(l[current_index: index])\n",
    "        current_index = index\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def iid_split(dataset, n_clients, frac, seed=1234):\n",
    "    \"\"\"\n",
    "    split classification dataset among `n_clients` in an IID fashion. The dataset is split as follow:\n",
    "\n",
    "    :param dataset:\n",
    "    :type dataset: torch.utils.Dataset\n",
    "    :param n_clients: number of clients\n",
    "    :param frac: fraction of dataset to use\n",
    "    :param seed:\n",
    "    :return: list (size `n_clients`) of subgroups, each subgroup is a list of indices.\n",
    "    \"\"\"\n",
    "    rng_seed = (seed if (seed is not None and seed >= 0) else int(time.time()))\n",
    "    rng = random.Random(rng_seed)\n",
    "    np.random.seed(rng_seed)\n",
    "\n",
    "    n_samples = int(len(dataset) * frac)\n",
    "    selected_indices = rng.sample(list(range(len(dataset))), n_samples)\n",
    "    rng.shuffle(selected_indices)\n",
    "\n",
    "    return iid_divide(selected_indices, n_clients)\n",
    "\n",
    "\n",
    "def by_labels_non_iid_split(dataset, n_classes, n_clients, n_clusters, alpha, frac, seed=1234):\n",
    "    \"\"\"\n",
    "    split classification dataset among `n_clients`. The dataset is split as follow:\n",
    "        1) classes are grouped into `n_clusters`\n",
    "        2) for each cluster `c`, samples are partitioned across clients using dirichlet distribution\n",
    "\n",
    "    Inspired by the split in \"Federated Learning with Matched Averaging\"__(https://arxiv.org/abs/2002.06440)\n",
    "\n",
    "    :param dataset:\n",
    "    :type dataset: torch.utils.Dataset\n",
    "    :param n_classes: number of classes present in `dataset`\n",
    "    :param n_clients: number of clients\n",
    "    :param n_clusters: number of clusters to consider; if it is `-1`, then `n_clusters = n_classes`\n",
    "    :param alpha: parameter controlling the diversity among clients\n",
    "    :param frac: fraction of dataset to use\n",
    "    :param seed:\n",
    "    :return: list (size `n_clients`) of subgroups, each subgroup is a list of indices.\n",
    "    \"\"\"\n",
    "    if n_clusters == -1:\n",
    "        n_clusters = n_classes\n",
    "\n",
    "    rng_seed = (seed if (seed is not None and seed >= 0) else int(time.time()))\n",
    "    rng = random.Random(rng_seed)\n",
    "    np.random.seed(rng_seed)\n",
    "\n",
    "    all_labels = list(range(n_classes))\n",
    "    rng.shuffle(all_labels)\n",
    "    clusters_labels = iid_divide(all_labels, n_clusters)\n",
    "\n",
    "    label2cluster = dict()  # maps label to its cluster\n",
    "    for group_idx, labels in enumerate(clusters_labels):\n",
    "        for label in labels:\n",
    "            label2cluster[label] = group_idx\n",
    "\n",
    "    # get subset\n",
    "    n_samples = int(len(dataset) * frac)\n",
    "    selected_indices = rng.sample(list(range(len(dataset))), n_samples)\n",
    "\n",
    "    clusters_sizes = np.zeros(n_clusters, dtype=int)\n",
    "    clusters = {k: [] for k in range(n_clusters)}\n",
    "    for idx in selected_indices:\n",
    "        _, label = dataset[idx]\n",
    "        group_id = label2cluster[label]\n",
    "        clusters_sizes[group_id] += 1\n",
    "        clusters[group_id].append(idx)\n",
    "\n",
    "    for _, cluster in clusters.items():\n",
    "        rng.shuffle(cluster)\n",
    "\n",
    "    clients_counts = np.zeros((n_clusters, n_clients), dtype=np.int64)  # number of samples by client from each cluster\n",
    "\n",
    "    for cluster_id in range(n_clusters):\n",
    "        weights = np.random.dirichlet(alpha=alpha * np.ones(n_clients))\n",
    "        clients_counts[cluster_id] = np.random.multinomial(clusters_sizes[cluster_id], weights)\n",
    "\n",
    "    clients_counts = np.cumsum(clients_counts, axis=1)\n",
    "\n",
    "    clients_indices = [[] for _ in range(n_clients)]\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_split = split_list_by_indices(clusters[cluster_id], clients_counts[cluster_id])\n",
    "\n",
    "        for client_id, indices in enumerate(cluster_split):\n",
    "            clients_indices[client_id] += indices\n",
    "\n",
    "    return clients_indices\n",
    "\n",
    "\n",
    "def pathological_non_iid_split(dataset, n_classes, n_clients, n_classes_per_client, frac=1, seed=1234):\n",
    "    \"\"\"\n",
    "    split classification dataset among `n_clients`. The dataset is split as follow:\n",
    "        1) sort the data by label\n",
    "        2) divide it into `n_clients * n_classes_per_client` shards, of equal size.\n",
    "        3) assign each of the `n_clients` with `n_classes_per_client` shards\n",
    "\n",
    "    Inspired by the split in\n",
    "     \"Communication-Efficient Learning of Deep Networks from Decentralized Data\"__(https://arxiv.org/abs/1602.05629)\n",
    "\n",
    "    :param dataset:\n",
    "    :type dataset: torch.utils.Dataset\n",
    "    :param n_classes: umber of classes present in `dataset`\n",
    "    :param n_clients: number of clients\n",
    "    :param n_classes_per_client:\n",
    "    :param frac: fraction of dataset to use\n",
    "    :param seed:\n",
    "    :return: list (size `n_clients`) of subgroups, each subgroup is a list of indices.\n",
    "    \"\"\"\n",
    "    rng_seed = (seed if (seed is not None and seed >= 0) else int(time.time()))\n",
    "    rng = random.Random(rng_seed)\n",
    "    np.random.seed(rng_seed)\n",
    "\n",
    "    # get subset\n",
    "    n_samples = int(len(dataset) * frac)\n",
    "    selected_indices = rng.sample(list(range(len(dataset))), n_samples)\n",
    "\n",
    "    label2index = {k: [] for k in range(n_classes)}\n",
    "    for idx in selected_indices:\n",
    "        _, label = dataset[idx]\n",
    "        label2index[label].append(idx)\n",
    "\n",
    "    sorted_indices = []\n",
    "    for label in label2index:\n",
    "        sorted_indices += label2index[label]\n",
    "\n",
    "    n_shards = n_clients * n_classes_per_client\n",
    "    shards = iid_divide(sorted_indices, n_shards)\n",
    "    random.shuffle(shards)\n",
    "    tasks_shards = iid_divide(shards, n_clients)\n",
    "\n",
    "    clients_indices = [[] for _ in range(n_clients)]\n",
    "    for client_id in range(n_clients):\n",
    "        for shard in tasks_shards[client_id]:\n",
    "            clients_indices[client_id] += shard\n",
    "\n",
    "    return clients_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-43f65dc8e44c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"./benchmark/mnist/data/\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.MNIST(\n",
    "    root=\"./benchmark/mnist/data/\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/disk1/hapq/sparseFL/gendata_new.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B202.191.58.206/mnt/disk1/hapq/sparseFL/gendata_new.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwhile\u001b[39;00m (\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B202.191.58.206/mnt/disk1/hapq/sparseFL/gendata_new.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     bl \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B202.191.58.206/mnt/disk1/hapq/sparseFL/gendata_new.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     res \u001b[39m=\u001b[39m by_labels_non_iid_split(training_data, n_classes\u001b[39m=\u001b[39;49mtotal_label, n_clients\u001b[39m=\u001b[39;49mnum_clients, n_clusters\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, alpha\u001b[39m=\u001b[39;49mdirichlet, frac\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m, seed\u001b[39m=\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandint(\u001b[39m1\u001b[39;49m,\u001b[39m100\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B202.191.58.206/mnt/disk1/hapq/sparseFL/gendata_new.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     dis_mtx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros([num_clients, total_label])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B202.191.58.206/mnt/disk1/hapq/sparseFL/gendata_new.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mfor\u001b[39;00m client_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(res)):\n",
      "\u001b[1;32m/mnt/disk1/hapq/sparseFL/gendata_new.ipynb Cell 4\u001b[0m in \u001b[0;36mby_labels_non_iid_split\u001b[0;34m(dataset, n_classes, n_clients, n_clusters, alpha, frac, seed)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.58.206/mnt/disk1/hapq/sparseFL/gendata_new.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m clusters \u001b[39m=\u001b[39m {k: [] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_clusters)}\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.58.206/mnt/disk1/hapq/sparseFL/gendata_new.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m selected_indices:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B202.191.58.206/mnt/disk1/hapq/sparseFL/gendata_new.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     _, label \u001b[39m=\u001b[39m dataset[idx]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.58.206/mnt/disk1/hapq/sparseFL/gendata_new.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m     group_id \u001b[39m=\u001b[39m label2cluster[label]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.58.206/mnt/disk1/hapq/sparseFL/gendata_new.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m     clusters_sizes[group_id] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torchvision/datasets/mnist.py:134\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    131\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torchvision/transforms/transforms.py:61\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 61\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torchvision/transforms/transforms.py:98\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m     91\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torchvision/transforms/functional.py:146\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[0;32m--> 146\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], \u001b[39mlen\u001b[39;49m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[1;32m    147\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m    148\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_clients=300\n",
    "# total_label=10\n",
    "\n",
    "labels = training_data.targets\n",
    "total_label = len(np.unique(labels))\n",
    "dirichlet = 0.1\n",
    "\n",
    "count = 0\n",
    "while (True):\n",
    "    bl = 1\n",
    "    res = by_labels_non_iid_split(training_data, n_classes=total_label, n_clients=num_clients, n_clusters=-1, alpha=dirichlet, frac=0.2, seed=random.randint(1,100))\n",
    "\n",
    "    dis_mtx = np.zeros([num_clients, total_label])\n",
    "    for client_id in range(len(res)):\n",
    "        for sample_id in res[client_id]:\n",
    "            label = training_data.targets[sample_id].item()\n",
    "            dis_mtx[client_id][label] += 1\n",
    "    \n",
    "    client_samples = np.sum(dis_mtx, 1)\n",
    "    for id in client_samples:\n",
    "        if(id == 0):\n",
    "            bl = 0\n",
    "            break\n",
    "    count += 1\n",
    "    if(bl == 1):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.heatmap(dis_mtx[:20], annot= True, cmap=\"coolwarm\")\n",
    "\n",
    "plt.savefig(f\"figures/{task}/{curve}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(dis_mtx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(dis_mtx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "\n",
    "for i in range(len(res)):\n",
    "    res_dict[i] = res[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Produce json file\n",
    "dataset = \"mnist\"\n",
    "\n",
    "dir_path = f\"./dataset_idx/{dataset}/sparse_dir{dirichlet}/{num_clients}client/\"\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "json.dump(res_dict, open(dir_path + f\"{dataset}_sparse.json\", \"w\"), indent=4, cls=NpEncoder)\n",
    "print(\"Output generated successfully\")\n",
    "\n",
    "# Produce stat file\n",
    "\n",
    "np.savetxt(dir_path + f\"{dataset}_sparse_stat.csv\", dis_mtx, delimiter=\",\", fmt=\"%d\")\n",
    "print(\"Stats generated successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f784b053654bb8129a3cb1aa1762d7834caeb9ba8691a85058f59d7796858ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
