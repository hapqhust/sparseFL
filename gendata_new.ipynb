{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def iid_divide(l, g):\n",
    "    \"\"\"\n",
    "    https://github.com/TalwalkarLab/leaf/blob/master/data/utils/sample.py\n",
    "    divide list `l` among `g` groups\n",
    "    each group has either `int(len(l)/g)` or `int(len(l)/g)+1` elements\n",
    "    returns a list of groups\n",
    "    \"\"\"\n",
    "    num_elems = len(l)\n",
    "    group_size = int(len(l) / g)\n",
    "    num_big_groups = num_elems - g * group_size\n",
    "    num_small_groups = g - num_big_groups\n",
    "    glist = []\n",
    "    for i in range(num_small_groups):\n",
    "        glist.append(l[group_size * i: group_size * (i + 1)])\n",
    "    bi = group_size * num_small_groups\n",
    "    group_size += 1\n",
    "    for i in range(num_big_groups):\n",
    "        glist.append(l[bi + group_size * i:bi + group_size * (i + 1)])\n",
    "    return glist\n",
    "\n",
    "\n",
    "def split_list_by_indices(l, indices):\n",
    "    \"\"\"\n",
    "    divide list `l` given indices into `len(indices)` sub-lists\n",
    "    sub-list `i` starts from `indices[i]` and stops at `indices[i+1]`\n",
    "    returns a list of sub-lists\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    current_index = 0\n",
    "    for index in indices:\n",
    "        res.append(l[current_index: index])\n",
    "        current_index = index\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def iid_split(dataset, n_clients, frac, seed=1234):\n",
    "    \"\"\"\n",
    "    split classification dataset among `n_clients` in an IID fashion. The dataset is split as follow:\n",
    "\n",
    "    :param dataset:\n",
    "    :type dataset: torch.utils.Dataset\n",
    "    :param n_clients: number of clients\n",
    "    :param frac: fraction of dataset to use\n",
    "    :param seed:\n",
    "    :return: list (size `n_clients`) of subgroups, each subgroup is a list of indices.\n",
    "    \"\"\"\n",
    "    rng_seed = (seed if (seed is not None and seed >= 0) else int(time.time()))\n",
    "    rng = random.Random(rng_seed)\n",
    "    np.random.seed(rng_seed)\n",
    "\n",
    "    n_samples = int(len(dataset) * frac)\n",
    "    selected_indices = rng.sample(list(range(len(dataset))), n_samples)\n",
    "    rng.shuffle(selected_indices)\n",
    "\n",
    "    return iid_divide(selected_indices, n_clients)\n",
    "\n",
    "\n",
    "def by_labels_non_iid_split(dataset, n_classes, n_clients, n_clusters, alpha, frac, seed=1234):\n",
    "    \"\"\"\n",
    "    split classification dataset among `n_clients`. The dataset is split as follow:\n",
    "        1) classes are grouped into `n_clusters`\n",
    "        2) for each cluster `c`, samples are partitioned across clients using dirichlet distribution\n",
    "\n",
    "    Inspired by the split in \"Federated Learning with Matched Averaging\"__(https://arxiv.org/abs/2002.06440)\n",
    "\n",
    "    :param dataset:\n",
    "    :type dataset: torch.utils.Dataset\n",
    "    :param n_classes: number of classes present in `dataset`\n",
    "    :param n_clients: number of clients\n",
    "    :param n_clusters: number of clusters to consider; if it is `-1`, then `n_clusters = n_classes`\n",
    "    :param alpha: parameter controlling the diversity among clients\n",
    "    :param frac: fraction of dataset to use\n",
    "    :param seed:\n",
    "    :return: list (size `n_clients`) of subgroups, each subgroup is a list of indices.\n",
    "    \"\"\"\n",
    "    if n_clusters == -1:\n",
    "        n_clusters = n_classes\n",
    "\n",
    "    rng_seed = (seed if (seed is not None and seed >= 0) else int(time.time()))\n",
    "    rng = random.Random(rng_seed)\n",
    "    np.random.seed(rng_seed)\n",
    "\n",
    "    all_labels = list(range(n_classes))\n",
    "    rng.shuffle(all_labels)\n",
    "    clusters_labels = iid_divide(all_labels, n_clusters)\n",
    "\n",
    "    label2cluster = dict()  # maps label to its cluster\n",
    "    for group_idx, labels in enumerate(clusters_labels):\n",
    "        for label in labels:\n",
    "            label2cluster[label] = group_idx\n",
    "\n",
    "    # get subset\n",
    "    n_samples = int(len(dataset) * frac)\n",
    "    selected_indices = rng.sample(list(range(len(dataset))), n_samples)\n",
    "\n",
    "    clusters_sizes = np.zeros(n_clusters, dtype=int)\n",
    "    clusters = {k: [] for k in range(n_clusters)}\n",
    "    for idx in selected_indices:\n",
    "        _, label = dataset[idx]\n",
    "        group_id = label2cluster[label]\n",
    "        clusters_sizes[group_id] += 1\n",
    "        clusters[group_id].append(idx)\n",
    "\n",
    "    for _, cluster in clusters.items():\n",
    "        rng.shuffle(cluster)\n",
    "\n",
    "    clients_counts = np.zeros((n_clusters, n_clients), dtype=np.int64)  # number of samples by client from each cluster\n",
    "\n",
    "    for cluster_id in range(n_clusters):\n",
    "        weights = np.random.dirichlet(alpha=alpha * np.ones(n_clients))\n",
    "        clients_counts[cluster_id] = np.random.multinomial(clusters_sizes[cluster_id], weights)\n",
    "\n",
    "    clients_counts = np.cumsum(clients_counts, axis=1)\n",
    "\n",
    "    clients_indices = [[] for _ in range(n_clients)]\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_split = split_list_by_indices(clusters[cluster_id], clients_counts[cluster_id])\n",
    "\n",
    "        for client_id, indices in enumerate(cluster_split):\n",
    "            clients_indices[client_id] += indices\n",
    "\n",
    "    return clients_indices\n",
    "\n",
    "\n",
    "def pathological_non_iid_split(dataset, n_classes, n_clients, n_classes_per_client, frac=1, seed=1234):\n",
    "    \"\"\"\n",
    "    split classification dataset among `n_clients`. The dataset is split as follow:\n",
    "        1) sort the data by label\n",
    "        2) divide it into `n_clients * n_classes_per_client` shards, of equal size.\n",
    "        3) assign each of the `n_clients` with `n_classes_per_client` shards\n",
    "\n",
    "    Inspired by the split in\n",
    "     \"Communication-Efficient Learning of Deep Networks from Decentralized Data\"__(https://arxiv.org/abs/1602.05629)\n",
    "\n",
    "    :param dataset:\n",
    "    :type dataset: torch.utils.Dataset\n",
    "    :param n_classes: umber of classes present in `dataset`\n",
    "    :param n_clients: number of clients\n",
    "    :param n_classes_per_client:\n",
    "    :param frac: fraction of dataset to use\n",
    "    :param seed:\n",
    "    :return: list (size `n_clients`) of subgroups, each subgroup is a list of indices.\n",
    "    \"\"\"\n",
    "    rng_seed = (seed if (seed is not None and seed >= 0) else int(time.time()))\n",
    "    rng = random.Random(rng_seed)\n",
    "    np.random.seed(rng_seed)\n",
    "\n",
    "    # get subset\n",
    "    n_samples = int(len(dataset) * frac)\n",
    "    selected_indices = rng.sample(list(range(len(dataset))), n_samples)\n",
    "\n",
    "    label2index = {k: [] for k in range(n_classes)}\n",
    "    for idx in selected_indices:\n",
    "        _, label = dataset[idx]\n",
    "        label2index[label].append(idx)\n",
    "\n",
    "    sorted_indices = []\n",
    "    for label in label2index:\n",
    "        sorted_indices += label2index[label]\n",
    "\n",
    "    n_shards = n_clients * n_classes_per_client\n",
    "    shards = iid_divide(sorted_indices, n_shards)\n",
    "    random.shuffle(shards)\n",
    "    tasks_shards = iid_divide(shards, n_clients)\n",
    "\n",
    "    clients_indices = [[] for _ in range(n_clients)]\n",
    "    for client_id in range(n_clients):\n",
    "        for shard in tasks_shards[client_id]:\n",
    "            clients_indices[client_id] += shard\n",
    "\n",
    "    return clients_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"./benchmark/mnist/data/\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.MNIST(\n",
    "    root=\"./benchmark/mnist/data/\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients=300\n",
    "# total_label=10\n",
    "\n",
    "labels = training_data.targets\n",
    "total_label = len(np.unique(labels))\n",
    "dirichlet = 100\n",
    "\n",
    "res = by_labels_non_iid_split(training_data, n_classes=total_label, n_clients=num_clients, n_clusters=-1, alpha=dirichlet, frac=0.2, seed=1)\n",
    "\n",
    "dis_mtx = np.zeros([num_clients, total_label])\n",
    "for client_id in range(len(res)):\n",
    "    for sample_id in res[client_id]:\n",
    "        label = training_data.targets[sample_id].item()\n",
    "        dis_mtx[client_id][label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 6., 3., ..., 6., 2., 4.],\n",
       "       [4., 2., 1., ..., 1., 7., 4.],\n",
       "       [4., 2., 4., ..., 3., 8., 2.],\n",
       "       ...,\n",
       "       [2., 8., 6., ..., 2., 2., 6.],\n",
       "       [3., 8., 1., ..., 4., 2., 4.],\n",
       "       [3., 5., 0., ..., 4., 3., 6.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38., 34., 43., 41., 40., 31., 54., 36., 33., 35., 46., 42., 31.,\n",
       "       40., 57., 28., 52., 26., 47., 30., 42., 39., 37., 44., 43., 46.,\n",
       "       35., 35., 35., 39., 43., 42., 32., 36., 42., 41., 39., 47., 40.,\n",
       "       33., 46., 36., 31., 46., 40., 46., 51., 46., 31., 52., 51., 36.,\n",
       "       42., 45., 51., 52., 46., 46., 34., 45., 41., 33., 42., 40., 42.,\n",
       "       30., 42., 39., 51., 34., 37., 30., 43., 45., 32., 36., 40., 32.,\n",
       "       32., 32., 42., 43., 41., 34., 45., 34., 51., 41., 47., 41., 40.,\n",
       "       37., 40., 44., 37., 41., 49., 44., 39., 39., 41., 32., 40., 40.,\n",
       "       47., 51., 38., 48., 28., 35., 36., 34., 44., 37., 36., 48., 38.,\n",
       "       42., 45., 26., 46., 35., 36., 39., 42., 40., 37., 31., 44., 42.,\n",
       "       36., 43., 41., 33., 47., 46., 34., 36., 43., 47., 41., 44., 36.,\n",
       "       37., 49., 42., 43., 37., 30., 31., 45., 37., 40., 29., 38., 55.,\n",
       "       48., 41., 45., 35., 40., 37., 36., 33., 49., 35., 48., 50., 28.,\n",
       "       42., 36., 38., 34., 39., 44., 46., 42., 30., 40., 52., 36., 34.,\n",
       "       31., 37., 39., 35., 43., 37., 60., 38., 37., 42., 43., 60., 46.,\n",
       "       52., 40., 49., 34., 44., 43., 44., 39., 40., 43., 29., 46., 43.,\n",
       "       43., 30., 52., 40., 47., 45., 34., 34., 43., 40., 31., 45., 27.,\n",
       "       39., 37., 38., 36., 42., 40., 37., 25., 41., 45., 42., 42., 33.,\n",
       "       39., 40., 36., 45., 39., 43., 28., 31., 40., 45., 36., 39., 33.,\n",
       "       32., 38., 44., 43., 39., 42., 39., 48., 38., 44., 40., 35., 44.,\n",
       "       55., 40., 48., 41., 27., 46., 29., 49., 37., 43., 41., 47., 41.,\n",
       "       40., 33., 35., 37., 29., 38., 43., 33., 45., 49., 34., 36., 36.,\n",
       "       40., 24., 46., 46., 42., 48., 46., 36., 44., 47., 41., 37., 35.,\n",
       "       29.])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dis_mtx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1200., 1327., 1149., 1222., 1191., 1072., 1203., 1271., 1154.,\n",
       "       1211.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dis_mtx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55867,\n",
       " 14805,\n",
       " 15321,\n",
       " 22745,\n",
       " 35804,\n",
       " 56618,\n",
       " 42013,\n",
       " 10906,\n",
       " 31674,\n",
       " 56052,\n",
       " 53852,\n",
       " 4571,\n",
       " 16882,\n",
       " 52937,\n",
       " 51950,\n",
       " 2037,\n",
       " 17869,\n",
       " 15014,\n",
       " 30511,\n",
       " 37847,\n",
       " 26554,\n",
       " 6299,\n",
       " 49450,\n",
       " 4756,\n",
       " 17725,\n",
       " 2084,\n",
       " 36975,\n",
       " 48596,\n",
       " 35946,\n",
       " 31701,\n",
       " 19167,\n",
       " 13725,\n",
       " 3540,\n",
       " 28981,\n",
       " 33176,\n",
       " 25551,\n",
       " 277,\n",
       " 45387]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict = {}\n",
    "\n",
    "for i in range(len(res)):\n",
    "    res_dict[i] = res[i]\n",
    "\n",
    "res_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output generated successfully\n",
      "Stats generated successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Produce json file\n",
    "dataset = \"mnist\"\n",
    "\n",
    "dir_path = f\"./dataset_idx/{dataset}/sparse_dir{dirichlet}/{num_clients}client/\"\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "json.dump(res_dict, open(dir_path + f\"{dataset}_sparse.json\", \"w\"), indent=4, cls=NpEncoder)\n",
    "print(\"Output generated successfully\")\n",
    "\n",
    "# Produce stat file\n",
    "\n",
    "np.savetxt(dir_path + f\"{dataset}_sparse_stat.csv\", dis_mtx, delimiter=\",\", fmt=\"%d\")\n",
    "print(\"Stats generated successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('longnd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f784b053654bb8129a3cb1aa1762d7834caeb9ba8691a85058f59d7796858ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
